//! Peephole optimizer for bytecode
//!
//! This module performs peephole optimizations on generated bytecode by replacing
//! common instruction sequences with fused instructions for better performance.

use crate::vm::Instr;

/// Optimize bytecode by fusing common instruction patterns
/// Returns (optimized_code, old_to_new_index_mapping)
/// The mapping allows updating function boundaries and entry points after optimization
/// The mapping vector has length code.len() + 1 to handle end-of-code boundaries
pub fn optimize(code: Vec<Instr>) -> (Vec<Instr>, Vec<usize>) {
    // Build old->new instruction index mapping
    // Add one extra entry for the "end" position (one past last instruction)
    let mut old_to_new: Vec<usize> = vec![0; code.len() + 1];
    let mut new_index = 0;
    let mut i = 0;

    while i < code.len() {
        old_to_new[i] = new_index;
        let fused = try_fuse_at(&code, i);
        match fused {
            Some((_, consumed)) => {
                // Mark all consumed instructions as mapping to the same new index
                for j in 0..consumed {
                    if i + j < code.len() {
                        old_to_new[i + j] = new_index;
                    }
                }
                new_index += 1;
                i += consumed;
            }
            None => {
                old_to_new[i] = new_index;
                new_index += 1;
                i += 1;
            }
        }
    }

    // Map the "end" position (one past last instruction)
    old_to_new[code.len()] = new_index;

    // Now optimize with the mapping
    let mut optimized = Vec::with_capacity(code.len());
    i = 0;

    while i < code.len() {
        let fused = try_fuse_at(&code, i);
        match fused {
            Some((fused_instr, consumed)) => {
                // Update jump target if this is a jump instruction
                let updated_instr = update_jump_target(fused_instr, &old_to_new);
                optimized.push(updated_instr);
                i += consumed;
            }
            None => {
                // Update jump target if this is a jump instruction
                let updated_instr = update_jump_target(code[i].clone(), &old_to_new);
                optimized.push(updated_instr);
                i += 1;
            }
        }
    }

    (optimized, old_to_new)
}

/// Optimize only unprotected ranges of bytecode, leaving protected ranges untouched.
/// Returns (optimized_code, old_to_new_index_mapping)
/// `protected_ranges` are ranges that should NOT be optimized (e.g., cached code).
/// Each range is (start, end) where end is exclusive.
pub fn optimize_with_protected_ranges(
    code: Vec<Instr>,
    protected_ranges: &[(usize, usize)],
) -> (Vec<Instr>, Vec<usize>) {
    // If no protected ranges, use standard optimization
    if protected_ranges.is_empty() {
        return optimize(code);
    }

    // Check if index is in a protected range
    let is_protected = |idx: usize| -> bool {
        protected_ranges
            .iter()
            .any(|(start, end)| idx >= *start && idx < *end)
    };

    // Build old->new instruction index mapping
    let mut old_to_new: Vec<usize> = vec![0; code.len() + 1];
    let mut optimized = Vec::with_capacity(code.len());
    let mut new_index = 0;
    let mut i = 0;

    while i < code.len() {
        old_to_new[i] = new_index;

        if is_protected(i) {
            // Protected range: copy instruction as-is
            optimized.push(code[i].clone());
            new_index += 1;
            i += 1;
        } else {
            // Unprotected range: try to fuse
            let fused = try_fuse_at(&code, i);
            match fused {
                Some((fused_instr, consumed)) => {
                    // Only fuse if all consumed instructions are also unprotected
                    let all_unprotected = (0..consumed).all(|j| !is_protected(i + j));
                    if all_unprotected {
                        for j in 0..consumed {
                            old_to_new[i + j] = new_index;
                        }
                        optimized.push(fused_instr);
                        new_index += 1;
                        i += consumed;
                    } else {
                        // Some instructions in fusion range are protected, copy as-is
                        optimized.push(code[i].clone());
                        new_index += 1;
                        i += 1;
                    }
                }
                None => {
                    optimized.push(code[i].clone());
                    new_index += 1;
                    i += 1;
                }
            }
        }
    }

    // Map the "end" position
    old_to_new[code.len()] = new_index;

    // Update all jump targets using the mapping
    for instr in &mut optimized {
        *instr = update_jump_target(instr.clone(), &old_to_new);
    }

    (optimized, old_to_new)
}

/// Update jump target in an instruction based on the old->new mapping
fn update_jump_target(instr: Instr, old_to_new: &[usize]) -> Instr {
    match instr {
        Instr::Jump(old_addr) => {
            let new_addr = if old_addr < old_to_new.len() {
                old_to_new[old_addr]
            } else {
                old_addr // Keep original if out of bounds (shouldn't happen)
            };
            Instr::Jump(new_addr)
        }
        Instr::JumpIfZero(old_addr) => {
            let new_addr = if old_addr < old_to_new.len() {
                old_to_new[old_addr]
            } else {
                old_addr
            };
            Instr::JumpIfZero(new_addr)
        }
        Instr::JumpIfNeI64(old_addr) => {
            let new_addr = if old_addr < old_to_new.len() {
                old_to_new[old_addr]
            } else {
                old_addr
            };
            Instr::JumpIfNeI64(new_addr)
        }
        Instr::JumpIfEqI64(old_addr) => {
            let new_addr = if old_addr < old_to_new.len() {
                old_to_new[old_addr]
            } else {
                old_addr
            };
            Instr::JumpIfEqI64(new_addr)
        }
        Instr::JumpIfLtI64(old_addr) => {
            let new_addr = if old_addr < old_to_new.len() {
                old_to_new[old_addr]
            } else {
                old_addr
            };
            Instr::JumpIfLtI64(new_addr)
        }
        Instr::JumpIfGtI64(old_addr) => {
            let new_addr = if old_addr < old_to_new.len() {
                old_to_new[old_addr]
            } else {
                old_addr
            };
            Instr::JumpIfGtI64(new_addr)
        }
        Instr::JumpIfLeI64(old_addr) => {
            let new_addr = if old_addr < old_to_new.len() {
                old_to_new[old_addr]
            } else {
                old_addr
            };
            Instr::JumpIfLeI64(new_addr)
        }
        Instr::JumpIfGeI64(old_addr) => {
            let new_addr = if old_addr < old_to_new.len() {
                old_to_new[old_addr]
            } else {
                old_addr
            };
            Instr::JumpIfGeI64(new_addr)
        }
        Instr::PushHandler(catch_ip, finally_ip) => {
            let map_ip = |ip: Option<usize>| {
                ip.map(|old_addr| {
                    if old_addr < old_to_new.len() {
                        old_to_new[old_addr]
                    } else {
                        old_addr
                    }
                })
            };
            Instr::PushHandler(map_ip(catch_ip), map_ip(finally_ip))
        }
        // Add other jump instructions as needed
        _ => instr, // No change for non-jump instructions
    }
}

/// Try to fuse instructions starting at index i
/// Returns Some((fused_instruction, num_instructions_consumed)) if successful
fn try_fuse_at(code: &[Instr], i: usize) -> Option<(Instr, usize)> {
    // Need at least 2 instructions for any fusion
    if i + 1 >= code.len() {
        return None;
    }

    let instr1 = &code[i];
    let instr2 = &code[i + 1];

    // Pattern 0 (3-instruction): Constant folding (Issue #3367)
    // PushI64(a) + PushI64(b) + ArithOp → PushI64(result)
    if let Instr::PushI64(a) = instr1 {
        if let Instr::PushI64(b) = instr2 {
            if i + 2 < code.len() {
                let instr3 = &code[i + 2];
                match instr3 {
                    Instr::AddI64 => return Some((Instr::PushI64(a.wrapping_add(*b)), 3)),
                    Instr::SubI64 => return Some((Instr::PushI64(a.wrapping_sub(*b)), 3)),
                    Instr::MulI64 => return Some((Instr::PushI64(a.wrapping_mul(*b)), 3)),
                    Instr::ModI64 if *b != 0 => return Some((Instr::PushI64(a.wrapping_rem(*b)), 3)),
                    _ => {}
                }
            }
        }
    }

    // PushF64(a) + PushF64(b) + ArithOp → PushF64(result)
    if let Instr::PushF64(a) = instr1 {
        if let Instr::PushF64(b) = instr2 {
            if i + 2 < code.len() {
                let instr3 = &code[i + 2];
                match instr3 {
                    Instr::AddF64 => return Some((Instr::PushF64(a + b), 3)),
                    Instr::SubF64 => return Some((Instr::PushF64(a - b), 3)),
                    Instr::MulF64 => return Some((Instr::PushF64(a * b), 3)),
                    Instr::DivF64 => return Some((Instr::PushF64(a / b), 3)),
                    _ => {}
                }
            }
        }
    }

    // Pattern 1 (3-instruction): LoadI64(x) + AddI64 + StoreI64(x) → IncVarI64(x)
    // Check this FIRST before the 2-instruction pattern, since it's more specific
    if let Instr::LoadI64(name) = instr1 {
        if i + 2 < code.len() {
            let instr3 = &code[i + 2];
            if matches!(instr2, Instr::AddI64) {
                if let Instr::StoreI64(store_name) = instr3 {
                    if store_name == name {
                        // cnt = cnt + 1  →  IncVarI64(cnt)
                        return Some((Instr::IncVarI64(name.clone()), 3));
                    }
                }
            }
            if matches!(instr2, Instr::SubI64) {
                if let Instr::StoreI64(store_name) = instr3 {
                    if store_name == name {
                        // cnt = cnt - 1  →  DecVarI64(cnt)
                        return Some((Instr::DecVarI64(name.clone()), 3));
                    }
                }
            }
        }

        // Pattern 2 (2-instruction): LoadI64(x) + {Add,Sub,Mul,Mod}I64 → Load{Op}I64(x)
        // Only if Pattern 1 didn't match
        match instr2 {
            Instr::AddI64 => {
                return Some((Instr::LoadAddI64(name.clone()), 2));
            }
            Instr::SubI64 => {
                return Some((Instr::LoadSubI64(name.clone()), 2));
            }
            Instr::MulI64 => {
                return Some((Instr::LoadMulI64(name.clone()), 2));
            }
            Instr::ModI64 => {
                return Some((Instr::LoadModI64(name.clone()), 2));
            }
            _ => {}
        }
    }

    // Pattern 3: NeI64 + JumpIfZero(addr) → JumpIfNeI64(addr)
    // Note: NeI64 pushes 1 if !=, 0 if ==
    // JumpIfZero jumps if value is 0
    // So: if (a != b) == 0 → if a == b → JumpIfEq
    // We want: if a != b → JumpIfNe
    // Actually, the logic is:
    //   NeI64: push (a != b ? 1 : 0)
    //   JumpIfZero: jump if top == 0
    // Combined: jump if (a != b) == 0 → jump if a == b
    // So NeI64 + JumpIfZero → JumpIfEqI64
    match (instr1, instr2) {
        (Instr::NeI64, Instr::JumpIfZero(addr)) => {
            // Jump if equal (because NeI64 returns 0 when equal)
            return Some((Instr::JumpIfEqI64(*addr), 2));
        }
        (Instr::EqI64, Instr::JumpIfZero(addr)) => {
            // Jump if not equal (because EqI64 returns 0 when not equal)
            return Some((Instr::JumpIfNeI64(*addr), 2));
        }
        (Instr::LtI64, Instr::JumpIfZero(addr)) => {
            // Jump if NOT less than (>= )
            return Some((Instr::JumpIfGeI64(*addr), 2));
        }
        (Instr::GtI64, Instr::JumpIfZero(addr)) => {
            // Jump if NOT greater than (<=)
            return Some((Instr::JumpIfLeI64(*addr), 2));
        }
        (Instr::LeI64, Instr::JumpIfZero(addr)) => {
            // Jump if NOT less or equal (>)
            return Some((Instr::JumpIfGtI64(*addr), 2));
        }
        (Instr::GeI64, Instr::JumpIfZero(addr)) => {
            // Jump if NOT greater or equal (<)
            return Some((Instr::JumpIfLtI64(*addr), 2));
        }
        _ => {}
    }

    None
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_load_add_fusion() {
        let code = vec![Instr::LoadI64("a".to_string()), Instr::AddI64];

        let (optimized, mapping) = optimize(code);

        assert_eq!(optimized.len(), 1);
        assert!(matches!(optimized[0], Instr::LoadAddI64(_)));
        assert_eq!(mapping.len(), 3); // 2 instructions + 1 end position
        assert_eq!(mapping[0], 0); // First instruction maps to 0
        assert_eq!(mapping[1], 0); // Second instruction also maps to 0 (fused)
        assert_eq!(mapping[2], 1); // End position maps to new length
    }

    #[test]
    fn test_inc_var_fusion() {
        let code = vec![
            Instr::LoadI64("cnt".to_string()),
            Instr::AddI64,
            Instr::StoreI64("cnt".to_string()),
        ];

        let (optimized, mapping) = optimize(code);

        assert_eq!(optimized.len(), 1);
        assert!(matches!(optimized[0], Instr::IncVarI64(_)));
        assert_eq!(mapping.len(), 4); // 3 instructions + 1 end position
        assert_eq!(mapping[0], 0);
        assert_eq!(mapping[1], 0);
        assert_eq!(mapping[2], 0); // All three fused to index 0
        assert_eq!(mapping[3], 1); // End position
    }

    #[test]
    fn test_compare_jump_fusion() {
        let code = vec![Instr::NeI64, Instr::JumpIfZero(100)];

        let (optimized, _mapping) = optimize(code);

        assert_eq!(optimized.len(), 1);
        assert!(matches!(optimized[0], Instr::JumpIfEqI64(_)));
    }

    #[test]
    fn test_no_fusion() {
        let code = vec![Instr::LoadI64("a".to_string()), Instr::PushI64(42)];

        let (optimized, mapping) = optimize(code);

        assert_eq!(optimized.len(), 2);
        assert_eq!(mapping.len(), 3); // 2 instructions + 1 end position
        assert_eq!(mapping[0], 0);
        assert_eq!(mapping[1], 1); // No fusion, sequential mapping
        assert_eq!(mapping[2], 2); // End position
    }

    #[test]
    fn test_optimize_with_protected_ranges() {
        // Code with some instructions that should be protected
        let code = vec![
            // Protected range [0, 2)
            Instr::LoadI64("a".to_string()), // 0 - protected
            Instr::AddI64,                   // 1 - protected
            // Unprotected range [2, 4)
            Instr::LoadI64("b".to_string()), // 2 - should fuse with 3
            Instr::SubI64,                   // 3
        ];

        let protected = vec![(0, 2)];
        let (optimized, mapping) = optimize_with_protected_ranges(code, &protected);

        // Protected instructions should remain as-is
        assert!(matches!(optimized[0], Instr::LoadI64(_)));
        assert!(matches!(optimized[1], Instr::AddI64));

        // Unprotected should be fused
        assert!(matches!(optimized[2], Instr::LoadSubI64(_)));

        assert_eq!(optimized.len(), 3);
        assert_eq!(mapping[0], 0);
        assert_eq!(mapping[1], 1);
        assert_eq!(mapping[2], 2);
        assert_eq!(mapping[3], 2); // Fused into same index
        assert_eq!(mapping[4], 3); // End position
    }

    #[test]
    fn test_protected_range_prevents_cross_boundary_fusion() {
        // Fusion pattern straddles protected boundary
        let code = vec![
            Instr::LoadI64("a".to_string()),  // 0 - unprotected, fusion start
            Instr::AddI64,                    // 1 - protected, fusion would continue here
            Instr::StoreI64("a".to_string()), // 2 - protected, fusion would end here
        ];

        let protected = vec![(1, 3)];
        let (optimized, _mapping) = optimize_with_protected_ranges(code, &protected);

        // Should NOT fuse because fusion crosses into protected range
        assert_eq!(optimized.len(), 3);
        assert!(matches!(optimized[0], Instr::LoadI64(_)));
        assert!(matches!(optimized[1], Instr::AddI64));
        assert!(matches!(optimized[2], Instr::StoreI64(_)));
    }

    #[test]
    fn test_empty_protected_ranges_uses_standard_optimize() {
        let code = vec![Instr::LoadI64("x".to_string()), Instr::AddI64];

        let (optimized, _) = optimize_with_protected_ranges(code, &[]);

        // Should fuse normally
        assert_eq!(optimized.len(), 1);
        assert!(matches!(optimized[0], Instr::LoadAddI64(_)));
    }

    // === Constant folding tests (Issue #3367) ===

    #[test]
    fn test_const_fold_i64_add() {
        let code = vec![Instr::PushI64(3), Instr::PushI64(4), Instr::AddI64];
        let (optimized, _) = optimize(code);
        assert_eq!(optimized.len(), 1);
        assert!(matches!(optimized[0], Instr::PushI64(7)));
    }

    #[test]
    fn test_const_fold_i64_sub() {
        let code = vec![Instr::PushI64(10), Instr::PushI64(3), Instr::SubI64];
        let (optimized, _) = optimize(code);
        assert_eq!(optimized.len(), 1);
        assert!(matches!(optimized[0], Instr::PushI64(7)));
    }

    #[test]
    fn test_const_fold_i64_mul() {
        let code = vec![Instr::PushI64(5), Instr::PushI64(6), Instr::MulI64];
        let (optimized, _) = optimize(code);
        assert_eq!(optimized.len(), 1);
        assert!(matches!(optimized[0], Instr::PushI64(30)));
    }

    #[test]
    fn test_const_fold_i64_mod() {
        let code = vec![Instr::PushI64(10), Instr::PushI64(3), Instr::ModI64];
        let (optimized, _) = optimize(code);
        assert_eq!(optimized.len(), 1);
        assert!(matches!(optimized[0], Instr::PushI64(1)));
    }

    #[test]
    fn test_const_fold_i64_mod_zero_no_fold() {
        // Division by zero should not be folded
        let code = vec![Instr::PushI64(10), Instr::PushI64(0), Instr::ModI64];
        let (optimized, _) = optimize(code);
        assert_eq!(optimized.len(), 3); // Not folded
    }

    #[test]
    fn test_const_fold_f64_add() {
        let code = vec![Instr::PushF64(1.5), Instr::PushF64(2.5), Instr::AddF64];
        let (optimized, _) = optimize(code);
        assert_eq!(optimized.len(), 1);
        match &optimized[0] {
            Instr::PushF64(v) => assert!((*v - 4.0).abs() < 1e-10),
            _ => panic!("Expected PushF64"),
        }
    }

    #[test]
    fn test_const_fold_f64_mul() {
        let code = vec![Instr::PushF64(3.0), Instr::PushF64(4.0), Instr::MulF64];
        let (optimized, _) = optimize(code);
        assert_eq!(optimized.len(), 1);
        match &optimized[0] {
            Instr::PushF64(v) => assert!((*v - 12.0).abs() < 1e-10),
            _ => panic!("Expected PushF64"),
        }
    }

    #[test]
    fn test_const_fold_f64_div() {
        let code = vec![Instr::PushF64(10.0), Instr::PushF64(4.0), Instr::DivF64];
        let (optimized, _) = optimize(code);
        assert_eq!(optimized.len(), 1);
        match &optimized[0] {
            Instr::PushF64(v) => assert!((*v - 2.5).abs() < 1e-10),
            _ => panic!("Expected PushF64"),
        }
    }
}
